{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# progress bars\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# limit to single device\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor  # uncomment to run on GPU\n",
    "\n",
    "# matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "fig_size = (7, 7)\n",
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['figure.figsize'] = fig_size\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['xtick.color'] = 'white'\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['ytick.color'] = 'white'\n",
    "%matplotlib inline\n",
    "\n",
    "# batching\n",
    "from random import shuffle\n",
    "\n",
    "def batch_generator(dataset, batch_size=5):\n",
    "    shuffle(dataset)\n",
    "    N_full_batches = len(dataset) // batch_size\n",
    "    for i in range(N_full_batches):\n",
    "        idx_from = batch_size * i\n",
    "        idx_to = batch_size * (i + 1)\n",
    "        imgs, masks = zip(*[(img, mask) for img, mask in dataset[idx_from:idx_to]])\n",
    "        yield imgs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.drive.extract_patches import get_data_training, get_data_testing\n",
    "\n",
    "batch_size = 32\n",
    "img_size = 64\n",
    "\n",
    "patches_imgs, patches_masks = get_data_training(N=2000, img_size=img_size)\n",
    "dataset_train = [(img, mask) for img, mask in zip(patches_imgs, patches_masks)]\n",
    "\n",
    "test_imgs, test_masks = get_data_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, kernel=2):\n",
    "        super(UnetDown, self).__init__()\n",
    "        self.down = nn.MaxPool2d(kernel)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.down(inputs)\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, \n",
    "                 kernel=2, stride=2, padding=(1, 1, 1, 1)):\n",
    "        super(UnetUp, self).__init__()\n",
    "        self.padding = padding\n",
    "        self.deconv = nn.ConvTranspose2d(in_size, out_size, \n",
    "                                         kernel, stride, padding=0)\n",
    "        self.norm = nn.BatchNorm2d(out_size)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = F.pad(inputs, self.padding)\n",
    "        outputs = self.deconv(outputs)\n",
    "        outputs = self.norm(outputs)\n",
    "        return self.act(outputs)\n",
    "\n",
    "\n",
    "class UnetConv(nn.Module):\n",
    "    def __init__(self, in_size, out_size, \n",
    "                 kernel=3, stride=1, padding=1,\n",
    "                 act=nn.ReLU()):\n",
    "        super(UnetConv, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_size, out_size, \n",
    "                              kernel, stride, padding)\n",
    "        self.norm = nn.BatchNorm2d(out_size)\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv(inputs)\n",
    "        outputs = self.norm(outputs)\n",
    "        if self.act is not None:\n",
    "            return self.act(outputs)\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "class UnetConc(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(UnetConc, self).__init__()\n",
    "\n",
    "        if dropout is not False and dropout > 0.:\n",
    "            self.dropout = torch.nn.Dropout()\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        \n",
    "    def forward(self, inputs1, inputs2):\n",
    "        x = torch.cat([inputs1, inputs2], 1)\n",
    "        \n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(Unet, self).__init__()\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.conv1 = UnetConv(3, 32)\n",
    "        self.conv2 = UnetConv(32, 32)\n",
    "        self.conv3 = UnetConv(32, 32)\n",
    "        self.conv4 = UnetConv(32, 32)\n",
    "        self.conv5 = UnetConv(32, 32)\n",
    "        self.conv6 = UnetConv(32, 32)\n",
    "        self.conv7 = UnetConv(32, 32)\n",
    "        self.conv8 = UnetConv(64, 32)\n",
    "        self.conv9 = UnetConv(32, 32)\n",
    "        self.conv10 = UnetConv(64, 32)\n",
    "        self.conv11 = UnetConv(32, 32)\n",
    "        self.conv12 = UnetConv(64, 32)\n",
    "        self.conv13 = UnetConv(32, 32)\n",
    "        self.conv14 = UnetConv(64, 32)\n",
    "        self.conv15 = UnetConv(32, 32)\n",
    "        self.conv16 = UnetConv(32, 1, act=None)\n",
    "    \n",
    "        self.down1 = UnetDown()\n",
    "        self.down2 = UnetDown()\n",
    "        self.down3 = UnetDown()\n",
    "        self.down4 = UnetDown()\n",
    "        self.down5 = UnetDown()\n",
    "\n",
    "        padding = (0, 0, 0, 0)\n",
    "        self.up1 = UnetUp(32, 32, padding=(0, 0, 0, 0))\n",
    "        self.up2 = UnetUp(32, 32, padding=(0, 0, 0, 0))\n",
    "        self.up3 = UnetUp(32, 32, padding=(0, 0, 0, 0))\n",
    "        self.up4 = UnetUp(32, 32, padding=(0, 0, 0, 0))\n",
    "        self.up5 = UnetUp(32, 32, padding=(0, 0, 0, 0))\n",
    "        \n",
    "        self.conc1 = UnetConc()\n",
    "        self.conc2 = UnetConc()\n",
    "        self.conc3 = UnetConc()\n",
    "        self.conc4 = UnetConc()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        if self.debug: print('conv1 : {}'.format(conv1.size()))\n",
    "        \n",
    "        down1 = self.down1(conv1)\n",
    "        conv2 = self.conv2(down1)\n",
    "        if self.debug: print('down1 : {}'.format(down1.size()))\n",
    "        if self.debug: print('conv2 : {}'.format(conv2.size()))\n",
    "\n",
    "        down2 = self.down2(conv2)\n",
    "        conv3 = self.conv3(down2)\n",
    "        if self.debug: print('down2 : {}'.format(down2.size()))\n",
    "        if self.debug: print('conv3 : {}'.format(conv3.size()))\n",
    "        \n",
    "        down3 = self.down3(conv3)\n",
    "        conv4 = self.conv4(down3)\n",
    "        if self.debug: print('down3 : {}'.format(down3.size()))\n",
    "        if self.debug: print('conv4 : {}'.format(conv4.size()))\n",
    "\n",
    "        down4 = self.down4(conv4)\n",
    "        conv5 = self.conv5(down4)\n",
    "        if self.debug: print('down4 : {}'.format(down4.size()))\n",
    "        if self.debug: print('conv5 : {}'.format(conv5.size()))\n",
    "\n",
    "        down5 = self.down5(conv5)\n",
    "        conv6 = self.conv6(down5)\n",
    "        if self.debug: print('down5 : {}'.format(down5.size()))\n",
    "        if self.debug: print('conv6 : {}'.format(conv6.size()))\n",
    "        \n",
    "        up1 = self.up1(conv6)\n",
    "        if self.debug: print('up1 : {}'.format(up1.size()))\n",
    "        conv7 = self.conv6(up1)\n",
    "        conc1 = self.conc1(conv7, down4)\n",
    "        conv8 = self.conv8(conc1)\n",
    "        if self.debug: print('conv7 : {}'.format(conv7.size()))\n",
    "        if self.debug: print('conc1 : {}'.format(conc1.size()))\n",
    "        if self.debug: print('conv8 : {}'.format(conv8.size()))\n",
    "\n",
    "        up2 = self.up2(conv8)\n",
    "        if self.debug: print('up2 : {}'.format(up2.size()))\n",
    "        conv9 = self.conv9(up2)\n",
    "        conc2 = self.conc2(conv9, down3)\n",
    "        conv10 = self.conv10(conc2)\n",
    "        if self.debug: print('conv9 : {}'.format(conv9.size()))\n",
    "        if self.debug: print('conc2 : {}'.format(conc2.size()))\n",
    "        if self.debug: print('conv10 : {}'.format(conv10.size()))\n",
    "            \n",
    "        up3 = self.up3(conv10)\n",
    "        if self.debug: print('up3 : {}'.format(up3.size()))\n",
    "        conv11 = self.conv11(up3)\n",
    "        conc3 = self.conc3(conv11, down2)\n",
    "        conv12 = self.conv12(conc3)\n",
    "        if self.debug: print('conv11 : {}'.format(conv11.size()))\n",
    "        if self.debug: print('conc3 : {}'.format(conc3.size()))\n",
    "        if self.debug: print('conv12 : {}'.format(conv12.size()))\n",
    "\n",
    "        up4 = self.up4(conv12)\n",
    "        if self.debug: print('up4 : {}'.format(up4.size()))\n",
    "        conv13 = self.conv13(up4)\n",
    "        conc4 = self.conc4(conv13, down1)\n",
    "        conv14 = self.conv14(conc4)\n",
    "        if self.debug: print('conv13 : {}'.format(conv13.size()))\n",
    "        if self.debug: print('conc4 : {}'.format(conc4.size()))\n",
    "        if self.debug: print('conv14 : {}'.format(conv14.size()))\n",
    "\n",
    "        up5 = self.up5(conv14)\n",
    "        if self.debug: print('up5 : {}'.format(up5.size()))\n",
    "        conv15 = self.conv15(up5)\n",
    "        conv16 = self.conv16(conv15)\n",
    "        if self.debug: print('conv15 : {}'.format(conv15.size()))\n",
    "        if self.debug: print('conv16 : {}'.format(conv16.size()))\n",
    "\n",
    "        outputs = nn.Sigmoid()(conv16)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "net = Unet(debug=False)\n",
    "\n",
    "inputs = Variable(torch.rand(1, 3, 64, 64))\n",
    "net(inputs).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.01, \n",
    "                       betas=(0.9, 0.995), eps=1e-05)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    \n",
    "    bgen = batch_generator(dataset_train, batch_size)\n",
    "    for idx, (imgs, masks) in enumerate(bgen):\n",
    "        imgs = np.asarray(imgs).reshape(batch_size, 3, img_size, img_size)\n",
    "        masks = np.asarray(masks).reshape(batch_size, 1, img_size, img_size)\n",
    "\n",
    "        inputs = Variable(torch.from_numpy(imgs).type(dtype))\n",
    "        targets = Variable(torch.from_numpy(masks).type(dtype))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(inputs)\n",
    "        \n",
    "        loss = criterion(pred, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss = loss.data[0]\n",
    "        \n",
    "        if idx % 25 == 0:\n",
    "            print('epoch {} -- batch {} -- loss {}'.format(epoch, idx, current_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
